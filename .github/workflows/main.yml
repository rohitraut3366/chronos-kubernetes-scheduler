name: CI/CD Pipeline

on:
  push:
    branches: [ master ]
  pull_request:
  # Allow manual triggering
  workflow_dispatch:

# Cancel old workflows when new commits are pushed to the same PR
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

env:
  GO_VERSION: '1.25'

jobs:
  integration-test:
    name: Full Integration Test
    runs-on: ubuntu-latest
    timeout-minutes: 30
    permissions:
      actions: read  # Required for cache access
    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Set up Go
      uses: actions/setup-go@v5
      with:
        go-version: ${{ env.GO_VERSION }}

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Install Helm
      run: |
        # Install Helm directly without needing GitHub token
        curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
        chmod 700 get_helm.sh
        ./get_helm.sh
        rm get_helm.sh

    - name: Install KIND
      run: |
        curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.23.0/kind-linux-amd64
        chmod +x ./kind
        sudo mv ./kind /usr/local/bin/kind
        
    - name: Create KIND cluster config
      run: |
        cat > kind-config.yaml << 'EOF'
        kind: Cluster
        apiVersion: kind.x-k8s.io/v1alpha4
        nodes:
        - role: control-plane
        - role: worker
        - role: worker
        EOF

    - name: Build Docker image with caching
      uses: docker/build-push-action@v6
      with:
        context: .
        file: build/Dockerfile
        platforms: linux/amd64
        push: false
        load: true
        tags: |
          chronos-kubernetes-scheduler:latest
          chronos-kubernetes-scheduler:integration-test
        cache-from: type=gha
        cache-to: type=gha,mode=max
        
    - name: Set up KIND multi-node cluster
      run: |
        # Dynamically determine Kubernetes version from go.mod
        K8S_VERSION=$(go list -m k8s.io/kubernetes | awk '{print $2}' | sed 's/^v//' | cut -d'.' -f1,2)
        KIND_IMAGE="kindest/node:v${K8S_VERSION}.0"
        
        echo "Detected Kubernetes version from go.mod: v${K8S_VERSION}"
        echo "Using KIND image: ${KIND_IMAGE}"
        
        # Verify the KIND image exists - FAIL if not available for consistency
        echo "üîç Verifying KIND image availability..."
        if docker pull ${KIND_IMAGE} >/dev/null 2>&1; then
          echo "‚úÖ KIND image ${KIND_IMAGE} is available"
        else
          echo "‚ùå KIND image ${KIND_IMAGE} not found!"
          echo "üí° This usually means:"
          echo "   1. Network/registry issue (temporary)"  
          echo "   2. KIND doesn't have image for v${K8S_VERSION}.0 yet"
          echo "   3. Version detection logic needs update"
          echo ""
          echo "üîß To fix: Check https://github.com/kubernetes-sigs/kind/releases"
          echo "   and verify v${K8S_VERSION}.0 image exists, or update go.mod"
          exit 1
        fi
        
        echo "Creating KIND multi-node cluster with ${KIND_IMAGE}..."
        kind create cluster --config kind-config.yaml --name chronos-test --image ${KIND_IMAGE}
        
        kind get kubeconfig --name chronos-test > /tmp/kubeconfig
        export KUBECONFIG=/tmp/kubeconfig
        kubectl config use-context kind-chronos-test
        
        echo "Waiting for nodes to be ready..."
        kubectl wait --for=condition=Ready nodes --all --timeout=300s
        
        echo "=== CLUSTER STATUS ==="
        kubectl cluster-info
        kubectl get nodes -o wide
        
        echo "Tagging image for KIND cluster..."
        docker tag chronos-kubernetes-scheduler:integration-test localhost/chronos-kubernetes-scheduler:integration-test
        
        echo "Loading scheduler image into KIND cluster..."
        kind load docker-image localhost/chronos-kubernetes-scheduler:integration-test --name chronos-test || {
          echo "‚ùå Image load failed!"
          echo "Available Docker images:"
          docker images | grep chronos-kubernetes-scheduler
          exit 1
        }
        
        echo "üéâ Image successfully loaded into KIND cluster!"

    - name: Deploy Chronos scheduler
      run: |
        export KUBECONFIG=/tmp/kubeconfig
        
        # Create namespace
        kubectl create namespace chronos-system
        
        # Create integration test values
        cat > values-ci.yaml << 'HELM_VALUES'
        replicaCount: 1
        
        image:
          registry: "localhost"
          repository: chronos-kubernetes-scheduler  
          tag: "integration-test"
          pullPolicy: Never
        
        scheduler:
          name: chronos-kubernetes-scheduler
          leaderElection:
            enabled: false  # Single replica in CI
          plugin:
            name: Chronos
            weight: 100
            jobDurationAnnotation: "scheduling.workload.io/expected-duration-seconds"
        
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 256Mi
        
        # Add tolerations for any taints
        tolerations:
        - operator: Exists
        
        # Simplified security context for CI
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
        HELM_VALUES
        
        # Debug before installation
        echo "=== DEBUGGING BEFORE HELM INSTALL ==="
        echo "Kubernetes cluster info:"
        kubectl cluster-info
        echo "Nodes:"
        kubectl get nodes -o wide
        echo "Available images in KIND:"
        docker images | grep -E "(chronos|scheduler)" || echo "No scheduler images found"
        echo "Helm values file:"
        cat values-ci.yaml
        
        echo "=== GENERATED SCHEDULER CONFIGURATION ==="
        helm template chronos-scheduler ./charts/chronos-kubernetes-scheduler --values values-ci.yaml --namespace chronos-system --show-only templates/configmap.yaml || {
          echo "‚ùå Helm template generation failed!"
          exit 1
        }
        
        echo "=== FULL CHART TEMPLATES DRY-RUN ==="
        helm template chronos-scheduler ./charts/chronos-kubernetes-scheduler --values values-ci.yaml --namespace chronos-system || {
          echo "‚ùå Helm template validation failed!"
          exit 1
        }
        
        # Install Helm chart with detailed logging
        echo "üöÄ Installing Helm chart..."
        helm upgrade --install chronos-scheduler \
          ./charts/chronos-kubernetes-scheduler \
          --namespace chronos-system \
          --values values-ci.yaml \
          --wait \
          --timeout 5m \
          --debug || {
          echo "‚ùå Helm installation failed!"
          echo "Helm status:"
          helm status chronos-scheduler -n chronos-system || true
          echo "Pod events:"
          kubectl get events -n chronos-system --sort-by='.lastTimestamp' || true
          exit 1
        }
        
        # Wait for scheduler pod to be ready
        echo "‚è≥ Waiting for scheduler pod to be ready..."
        
        # First check all pods to see what labels exist
        echo "Debug: All pods in chronos-system:"
        kubectl get pods -n chronos-system --show-labels
        
        # Wait for any pod in the namespace to be ready (more reliable for single deployment)
        kubectl wait --for=condition=Ready pod --all -n chronos-system --timeout=180s || {
          echo "‚ùå Scheduler pod failed to become ready!"
          echo "Pod status:"
          kubectl get pods -n chronos-system -o wide
          echo "Pod describe:"
          kubectl describe pods -n chronos-system
          echo "Pod logs:"
          kubectl logs -n chronos-system -l app.kubernetes.io/name=chronos-kubernetes-scheduler --tail=50 || true
          exit 1
        }
        
        # Show scheduler status
        echo "‚úÖ Scheduler pod is ready!"
        kubectl get pods -n chronos-system -o wide
        kubectl logs -n chronos-system -l app.kubernetes.io/name=chronos-kubernetes-scheduler --tail=20

    - name: Run integration tests
      run: |
        export KUBECONFIG=/tmp/kubeconfig
        
        # Create test namespace
        kubectl create namespace integration-test
        
        # Create test pods
        cat > integration-test-pods.yaml << 'TEST_PODS'
        apiVersion: v1
        kind: Pod
        metadata:
          name: test-short-job
          namespace: integration-test
          annotations:
            scheduling.workload.io/expected-duration-seconds: "60"
        spec:
          schedulerName: chronos-kubernetes-scheduler
          containers:
          - name: worker
            image: alpine:latest
            command: ["sh", "-c", "echo 'Short job starting'; sleep 60; echo 'Short job completed'"]
          restartPolicy: Never
        
        ---
        apiVersion: v1
        kind: Pod
        metadata:
          name: test-decimal-duration
          namespace: integration-test
          annotations:
            scheduling.workload.io/expected-duration-seconds: "120.5"
        spec:
          schedulerName: chronos-kubernetes-scheduler
          containers:
          - name: worker
            image: busybox:latest
            command: ["sh", "-c", "echo 'Decimal duration job'; sleep 120; echo 'Completed'"]
          restartPolicy: Never
        
        ---
        apiVersion: v1
        kind: Pod
        metadata:
          name: test-no-annotation
          namespace: integration-test
        spec:
          schedulerName: chronos-kubernetes-scheduler
          containers:
          - name: worker
            image: nginx:alpine
            command: ["sh", "-c", "echo 'No annotation job'; sleep 30; echo 'Completed'"]
          restartPolicy: Never
        TEST_PODS
        
        # Apply test pods  
        kubectl apply -f integration-test-pods.yaml
        
        # Wait for pods to be scheduled
        sleep 15
        
        # Check results
        echo "=== POD SCHEDULING RESULTS ==="
        kubectl get pods -n integration-test -o wide
        
        echo "=== SCHEDULER LOGS ==="
        kubectl logs -n chronos-system -l app.kubernetes.io/name=chronos-kubernetes-scheduler --tail=50
        
        echo "=== SCHEDULING EVENTS ==="
        kubectl get events -n integration-test --field-selector reason=Scheduled
        
        echo "=== FAILED SCHEDULING EVENTS ==="  
        kubectl get events -n integration-test --field-selector reason=FailedScheduling

    - name: Print scheduler logs
      run: |
        export KUBECONFIG=/tmp/kubeconfig
        
        echo "=== SCHEDULER DEPLOYMENT STATUS ==="
        kubectl get pods -n chronos-system -o wide
        kubectl get deployment -n chronos-system
        
        # Get all scheduler pod names
        SCHEDULER_PODS=$(kubectl get pods -n chronos-system -l app.kubernetes.io/name=chronos-kubernetes-scheduler -o jsonpath='{.items[*].metadata.name}')
        echo "Scheduler pods found: $SCHEDULER_PODS"
        
        # Find the leader by checking the lease resource or fallback to single pod
        echo "=== IDENTIFYING SCHEDULER LEADER ==="
        
        # Check if we have single replica (no leader election needed)
        POD_COUNT=$(echo $SCHEDULER_PODS | wc -w)
        echo "Number of scheduler pods: $POD_COUNT"
        
        if [ "$POD_COUNT" -eq 1 ]; then
          LEADER_POD=$(echo $SCHEDULER_PODS | awk '{print $1}')
          echo "‚úÖ Single replica deployment - using pod: $LEADER_POD"
        else
          echo "Multiple replicas detected - checking for leader election lease..."
          
          # Check for leases (standard scheduler lease locations)
          LEASE_NAME="chronos-kubernetes-scheduler" 
          LEADER_IDENTITY=""
          
          # Try kube-system first (default scheduler lease location)
          LEADER_IDENTITY=$(kubectl get lease $LEASE_NAME -n kube-system -o jsonpath='{.spec.holderIdentity}' 2>/dev/null || echo "")
          
          # Try chronos-system if not found in kube-system
          if [ -z "$LEADER_IDENTITY" ]; then
            LEADER_IDENTITY=$(kubectl get lease $LEASE_NAME -n chronos-system -o jsonpath='{.spec.holderIdentity}' 2>/dev/null || echo "")
          fi
          
          # Show available leases for debugging
          echo "Available leases:"
          kubectl get leases -n kube-system | grep -i chronos || echo "  No chronos leases in kube-system"
          kubectl get leases -n chronos-system | grep -i chronos || echo "  No chronos leases in chronos-system"
          
          if [ -n "$LEADER_IDENTITY" ]; then
            echo "Leader identity from lease: $LEADER_IDENTITY"
            # Extract pod name (lease identity format: podname_namespace_uid)
            LEADER_POD=$(echo $LEADER_IDENTITY | cut -d'_' -f1)
            echo "‚úÖ Leader pod identified via lease: $LEADER_POD"
            
            # Verify pod exists in our list
            if echo "$SCHEDULER_PODS" | grep -q "$LEADER_POD"; then
              echo "‚úÖ Leader pod confirmed in scheduler pod list"
            else
              echo "‚ö†Ô∏è Leader pod from lease not found in current pods, using first pod"
              LEADER_POD=$(echo $SCHEDULER_PODS | awk '{print $1}')
            fi
          else
            echo "‚ö†Ô∏è No lease found, using first pod (leader election may be disabled)"
            LEADER_POD=$(echo $SCHEDULER_PODS | awk '{print $1}')
          fi
        fi
        
        echo "=== LEADER SCHEDULER LOGS ($LEADER_POD) ==="
        kubectl logs -n chronos-system $LEADER_POD
        
        echo "=== SCHEDULER STARTUP LOGS ==="
        kubectl logs -n chronos-system $LEADER_POD | grep -E "(Chronos|plugin|scheduler)" | head -20
        
        echo "=== CHRONOS PLUGIN INITIALIZATION ==="
        kubectl logs -n chronos-system $LEADER_POD | grep -i "Initializing Chronos plugin" || echo "No Chronos plugin initialization found"
        
        echo "=== POD SCHEDULING ACTIVITY ==="
        kubectl logs -n chronos-system $LEADER_POD | grep -E "(Attempting to schedule|evaluating|scoring)" | head -10
        
        echo "=== CHRONOS_SCORE LOGS FROM LEADER ==="
        kubectl logs -n chronos-system $LEADER_POD | grep "CHRONOS_SCORE:" || echo "No CHRONOS_SCORE logs found"
        
        echo "=== NORMALIZED SCORE LOGS FROM LEADER ==="  
        kubectl logs -n chronos-system $LEADER_POD | grep "NormalizedScore:" || echo "No NormalizedScore logs found"
        
        echo "=== SUCCESSFULLY BOUND LOGS FROM LEADER ==="
        kubectl logs -n chronos-system $LEADER_POD | grep "Successfully bound" || echo "No successful binding logs found"
        
        echo "=== TEST POD SCHEDULER VERIFICATION ==="
        for pod in $(kubectl get pods -n integration-test -o name); do
          POD_NAME=$(echo $pod | cut -d'/' -f2)
          SCHEDULER=$(kubectl get pod -n integration-test $POD_NAME -o jsonpath='{.spec.schedulerName}')
          ANNOTATIONS=$(kubectl get pod -n integration-test $POD_NAME -o jsonpath='{.metadata.annotations}' || echo "none")
          echo "Pod: $POD_NAME | Scheduler: $SCHEDULER | Annotations: $ANNOTATIONS"
        done
        
        echo "=== BASIC VALIDATION (INFORMATIONAL) ==="
        CHRONOS_PODS=$(kubectl get pods -n integration-test -o jsonpath='{.items[?(@.spec.schedulerName=="chronos-kubernetes-scheduler")].metadata.name}' | wc -w)
        TOTAL_PODS=$(kubectl get pods -n integration-test --no-headers | wc -l)
        echo "Pods scheduled by Chronos: $CHRONOS_PODS/$TOTAL_PODS"
        
        CHRONOS_SCORE_COUNT=$(kubectl logs -n chronos-system $LEADER_POD | grep -c "CHRONOS_SCORE:" || echo "0")
        echo "CHRONOS_SCORE log entries: $CHRONOS_SCORE_COUNT"
        
        NORMALIZED_SCORE_COUNT=$(kubectl logs -n chronos-system $LEADER_POD | grep -c "NormalizedScore:" || echo "0") 
        echo "NormalizedScore log entries: $NORMALIZED_SCORE_COUNT"
        
        BINDING_COUNT=$(kubectl logs -n chronos-system $LEADER_POD | grep -c "Successfully bound" || echo "0")
        echo "Successfully bound events: $BINDING_COUNT"
        
        echo "=== NODE ANALYSIS ==="
        NODE_COUNT=$(kubectl get nodes --no-headers | wc -l)
        READY_NODE_COUNT=$(kubectl get nodes --no-headers | grep -c " Ready ")
        echo "Total nodes in cluster: $NODE_COUNT (Ready: $READY_NODE_COUNT)"
        
        kubectl get nodes -o wide
        
        if [ "$READY_NODE_COUNT" -eq 1 ]; then
          echo "‚ÑπÔ∏è  Single node cluster detected - this explains why no scoring logs appear"
          echo "   Kubernetes skips scoring phase when there's only one feasible node"
          echo "   This is normal behavior and indicates our scheduler is working correctly"
        elif [ "$READY_NODE_COUNT" -gt 1 ]; then
          echo "üéØ Multi-node cluster detected - CHRONOS_SCORE logs should now appear!"
          echo "   Expected: evaluatedNodes=$READY_NODE_COUNT, with scoring comparisons between nodes"
          echo "   Our custom scheduler plugin should rank nodes using bin-packing logic"
          
          # Verify we have multi-node evaluation in logs
          MULTI_NODE_EVALUATIONS=$(kubectl logs -n chronos-system $LEADER_POD | grep "CHRONOS_SCORE:" | wc -l)
          if [ "$MULTI_NODE_EVALUATIONS" -gt 3 ]; then
            echo "‚úÖ Multi-node evaluation detected: $MULTI_NODE_EVALUATIONS CHRONOS_SCORE entries"
            echo "   This confirms our scheduler is evaluating multiple nodes!"
          fi
        else
          echo "‚ö†Ô∏è  No ready nodes detected - cluster may still be initializing"
        fi
        
        echo "=== CRITICAL VALIDATION CHECKS ==="
        
        # Check 1: Scheduler pod must be running
        if [ -z "$LEADER_POD" ]; then
          echo "‚ùå FAIL: No scheduler leader pod identified"
          exit 1
        fi
        
        SCHEDULER_STATUS=$(kubectl get pod -n chronos-system $LEADER_POD -o jsonpath='{.status.phase}')
        if [ "$SCHEDULER_STATUS" != "Running" ]; then
          echo "‚ùå FAIL: Scheduler pod is not running (status: $SCHEDULER_STATUS)"
          exit 1
        fi
        echo "‚úÖ PASS: Scheduler pod is running"
        
        # Check 2: If multiple replicas, ensure lease exists (leader election working)
        if [ "$POD_COUNT" -gt 1 ]; then
          # We should have found a leader via lease for multi-replica deployments
          if [ -z "$LEADER_IDENTITY" ]; then
            echo "‚ùå FAIL: Multiple scheduler replicas but no leader election lease found"
            echo "This indicates leader election is not working properly"
            exit 1
          fi
          echo "‚úÖ PASS: Leader election working (lease found: $LEADER_IDENTITY)"
        else
          echo "‚úÖ PASS: Single replica deployment (no leader election needed)"
        fi
        
        echo "üéâ All critical infrastructure checks passed!"
        echo "üìä Log metrics above are informational only and don't affect build status"

    - name: Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-scheduler-sim
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install Python dependencies
      run: |
        # Pre-install dependencies for speed
        pip3 install --user PyYAML
        
    - name: Run Scheduler Logic Simulations
      run: |
        export KUBECONFIG=/tmp/kubeconfig
        
        echo "üß™ Running Chronos Scheduler Logic Simulations..."
        echo "This validates our bin-packing and consolidation logic with real scenarios"
        
        cd test-workloads
        
        # Add debug info before running simulations
        echo "=== PRE-SIMULATION DEBUG INFO ==="
        kubectl get nodes -o wide
        kubectl get pods -A | grep -v "Completed\|Succeeded" | head -10
        kubectl logs -n chronos-system -l app.kubernetes.io/name=chronos-kubernetes-scheduler --tail=10 || echo "No scheduler logs yet"
        echo "=== STARTING SIMULATIONS ==="
        
        timeout 300s python3 run-simulations.py --config simulations.yaml --kubeconfig /tmp/kubeconfig || {
          EXIT_CODE=$?
          
          if [ $EXIT_CODE -eq 124 ]; then
            echo "‚ö†Ô∏è Simulations timed out after 5 minutes"
            echo "This suggests scheduling decisions are taking too long"
          else
            echo "‚ùå Simulations failed with exit code: $EXIT_CODE"
            echo "This indicates our scheduling logic doesn't match expected behavior"
          fi
          
          echo "üîç Recent scheduler logs for debugging:"
          kubectl logs -n chronos-system -l app.kubernetes.io/name=chronos-kubernetes-scheduler --tail=50
          
          echo "üîç Simulation namespace pods status:"
          kubectl get pods -n simulation-test -o wide || echo "No simulation pods found"
          
          echo "üîç Recent events in simulation namespace:"
          kubectl get events -n simulation-test --sort-by='.lastTimestamp' | tail -20 || echo "No simulation events found"
          
          exit $EXIT_CODE
        }
        
        echo "‚úÖ Scheduler logic simulations completed successfully!"

    - name: Cleanup
      if: always()
      run: |
        # Show final status
        export KUBECONFIG=/tmp/kubeconfig
        kubectl get pods -A || true

  docker-build:
    name: Build & Push Docker Image
    needs: integration-test
    runs-on: ubuntu-latest
    # Only build Docker images on master pushes (not PRs or main branch)
    if: github.ref == 'refs/heads/master' && github.event_name == 'push'
    permissions:
      contents: read
      packages: write  # Required for GHCR push
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v5
        
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ github.repository_owner }}/chronos-kubernetes-scheduler
          tags: |
            type=raw,value=${{ github.sha }}
            type=raw,value=latest
      
      - name: Build and push Docker image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: build/Dockerfile
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
