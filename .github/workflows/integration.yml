name: Integration Tests

on:
  push:
    branches: [ main, master ]
  pull_request:
  # Allow manual triggering
  workflow_dispatch:

# Cancel old workflows when new commits are pushed to the same PR
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

env:
  GO_VERSION: '1.22'

jobs:
  integration-test:
    name: Full Integration Test
    runs-on: ubuntu-latest
    timeout-minutes: 30
    permissions:
      actions: read  # Required for cache access
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Go
      uses: actions/setup-go@v4
      with:
        go-version: ${{ env.GO_VERSION }}

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Install Helm
      run: |
        # Install Helm directly without needing GitHub token
        curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
        chmod 700 get_helm.sh
        ./get_helm.sh
        rm get_helm.sh

    - name: Set up K3s multi-node cluster
      run: |
        # Install K3s server (master node)
        curl -sfL https://get.k3s.io | sh -s - \
          --write-kubeconfig-mode 644 \
          --disable traefik \
          --disable servicelb \
          --disable metrics-server \
          --node-name k3s-server

        # Wait for K3s server to be ready
        sudo chmod 644 /etc/rancher/k3s/k3s.yaml
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        # Wait for server node to be ready
        timeout 60s bash -c 'until kubectl get nodes | grep -q Ready; do sleep 5; done'
        
        # Get the node token and server info for agents
        K3S_TOKEN=$(sudo cat /var/lib/rancher/k3s/server/node-token)
        K3S_SERVER_IP=$(hostname -I | cut -d' ' -f1)
        
        echo "Server ready, adding agent nodes..."
        
        # Add second node using Docker container
        docker run -d \
          --name k3s-agent-1 \
          --privileged \
          --network host \
          --pid host \
          --ipc host \
          --tmpfs /run \
          --tmpfs /var/run \
          -v /lib/modules:/lib/modules:ro \
          rancher/k3s:latest \
          agent \
          --server https://${K3S_SERVER_IP}:6443 \
          --token ${K3S_TOKEN} \
          --node-name k3s-agent-1
        
        # Add third node for more diverse scheduling
        docker run -d \
          --name k3s-agent-2 \
          --privileged \
          --network host \
          --pid host \
          --ipc host \
          --tmpfs /run \
          --tmpfs /var/run \
          -v /lib/modules:/lib/modules:ro \
          rancher/k3s:latest \
          agent \
          --server https://${K3S_SERVER_IP}:6443 \
          --token ${K3S_TOKEN} \
          --node-name k3s-agent-2
        
        # Wait for all nodes to be ready (up to 3 minutes)
        echo "Waiting for all nodes to be ready..."
        for i in {1..36}; do
          READY_NODES=$(kubectl get nodes --no-headers | grep -c " Ready ")
          TOTAL_NODES=$(kubectl get nodes --no-headers | wc -l)
          echo "Ready nodes: $READY_NODES/$TOTAL_NODES"
          
          if [ "$READY_NODES" -ge 3 ]; then
            echo "✅ Multi-node cluster ready with $READY_NODES nodes!"
            break
          fi
          
          if [ "$i" -eq 36 ]; then
            echo "⚠️ Timeout waiting for all nodes, continuing with available nodes"
          fi
          
          sleep 5
        done
        
        echo "=== FINAL CLUSTER STATUS ==="
        kubectl cluster-info
        kubectl get nodes -o wide
        
        echo "=== NODE DETAILS ==="
        kubectl describe nodes

    - name: Build Docker image with caching
      uses: docker/build-push-action@v5
      with:
        context: .
        file: build/Dockerfile
        platforms: linux/amd64
        push: false
        load: true
        tags: chronos-kubernetes-scheduler:latest
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Import image to K3s
      run: |
        # Verify local build
        echo "🔍 Verifying local Docker build..."
        docker images | grep chronos-kubernetes-scheduler
        
        # Tag for integration tests with dummy registry
        echo "🏷️ Tagging image for integration tests..."
        docker tag chronos-kubernetes-scheduler:latest dummy/chronos-kubernetes-scheduler:integration-test
        
        # Import into K3s with better error handling
        echo "📦 Importing image into K3s..."
        docker save dummy/chronos-kubernetes-scheduler:integration-test | sudo k3s ctr images import -
        
        # Verify import worked
        echo "✅ Verifying image import..."
        sudo k3s ctr images list | grep chronos-kubernetes-scheduler || {
          echo "❌ Image import failed!"
          echo "Available K3s images:"
          sudo k3s ctr images list
          exit 1
        }
        
        echo "🎉 Image successfully imported to K3s!"

    - name: Deploy Chronos scheduler
      run: |
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        # Create namespace
        kubectl create namespace chronos-system
        
        # Create integration test values
        cat > values-ci.yaml << 'HELM_VALUES'
        replicaCount: 1
        
        image:
          registry: dummy
          repository: chronos-kubernetes-scheduler  
          tag: "integration-test"
          pullPolicy: Never
        
        scheduler:
          name: chronos-kubernetes-scheduler
          leaderElection:
            enabled: false  # Single replica in CI
          plugin:
            name: Chronos
            weight: 100
            jobDurationAnnotation: "scheduling.workload.io/expected-duration-seconds"
        
        # Ensure we see our custom logs (klog.Infof requires level 2+)
        logging:
          level: 4
          format: text
        
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 256Mi
        
        # Add tolerations for any taints
        tolerations:
        - operator: Exists
        
        # Simplified security context for CI
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
        HELM_VALUES
        
        # Debug before installation
        echo "=== DEBUGGING BEFORE HELM INSTALL ==="
        echo "Kubernetes cluster info:"
        kubectl cluster-info
        echo "Nodes:"
        kubectl get nodes -o wide
        echo "Available images in K3s:"
        sudo k3s ctr images list | grep -E "(chronos|scheduler)" || echo "No scheduler images found"
        echo "Helm values file:"
        cat values-ci.yaml
        
        echo "=== GENERATED SCHEDULER CONFIGURATION ==="
        helm template chronos-scheduler ./charts/chronos-kubernetes-scheduler --values values-ci.yaml --namespace chronos-system --show-only templates/configmap.yaml || {
          echo "❌ Helm template generation failed!"
          exit 1
        }
        
        echo "=== FULL CHART TEMPLATES DRY-RUN ==="
        helm template chronos-scheduler ./charts/chronos-kubernetes-scheduler --values values-ci.yaml --namespace chronos-system || {
          echo "❌ Helm template validation failed!"
          exit 1
        }
        
        # Install Helm chart with detailed logging
        echo "🚀 Installing Helm chart..."
        helm upgrade --install chronos-scheduler \
          ./charts/chronos-kubernetes-scheduler \
          --namespace chronos-system \
          --values values-ci.yaml \
          --wait \
          --timeout 5m \
          --debug || {
          echo "❌ Helm installation failed!"
          echo "Helm status:"
          helm status chronos-scheduler -n chronos-system || true
          echo "Pod events:"
          kubectl get events -n chronos-system --sort-by='.lastTimestamp' || true
          exit 1
        }
        
        # Wait for scheduler pod to be ready
        echo "⏳ Waiting for scheduler pod to be ready..."
        
        # First check all pods to see what labels exist
        echo "Debug: All pods in chronos-system:"
        kubectl get pods -n chronos-system --show-labels
        
        # Wait for any pod in the namespace to be ready (more reliable for single deployment)
        kubectl wait --for=condition=Ready pod --all -n chronos-system --timeout=180s || {
          echo "❌ Scheduler pod failed to become ready!"
          echo "Pod status:"
          kubectl get pods -n chronos-system -o wide
          echo "Pod describe:"
          kubectl describe pods -n chronos-system
          echo "Pod logs:"
          kubectl logs -n chronos-system -l app.kubernetes.io/name=chronos-kubernetes-scheduler --tail=50 || true
          exit 1
        }
        
        # Show scheduler status
        echo "✅ Scheduler pod is ready!"
        kubectl get pods -n chronos-system -o wide
        kubectl logs -n chronos-system -l app.kubernetes.io/name=chronos-kubernetes-scheduler --tail=20

    - name: Run integration tests
      run: |
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        # Create test namespace
        kubectl create namespace integration-test
        
        # Create test pods
        cat > integration-test-pods.yaml << 'TEST_PODS'
        apiVersion: v1
        kind: Pod
        metadata:
          name: test-short-job
          namespace: integration-test
          annotations:
            scheduling.workload.io/expected-duration-seconds: "60"
        spec:
          schedulerName: chronos-kubernetes-scheduler
          containers:
          - name: worker
            image: alpine:latest
            command: ["sh", "-c", "echo 'Short job starting'; sleep 60; echo 'Short job completed'"]
          restartPolicy: Never
        
        ---
        apiVersion: v1
        kind: Pod
        metadata:
          name: test-decimal-duration
          namespace: integration-test
          annotations:
            scheduling.workload.io/expected-duration-seconds: "120.5"
        spec:
          schedulerName: chronos-kubernetes-scheduler
          containers:
          - name: worker
            image: busybox:latest
            command: ["sh", "-c", "echo 'Decimal duration job'; sleep 120; echo 'Completed'"]
          restartPolicy: Never
        
        ---
        apiVersion: v1
        kind: Pod
        metadata:
          name: test-no-annotation
          namespace: integration-test
        spec:
          schedulerName: chronos-kubernetes-scheduler
          containers:
          - name: worker
            image: nginx:alpine
            command: ["sh", "-c", "echo 'No annotation job'; sleep 30; echo 'Completed'"]
          restartPolicy: Never
        TEST_PODS
        
        # Apply test pods
        kubectl apply -f integration-test-pods.yaml
        
        # Wait for pods to be scheduled
        sleep 15
        
        # Check results
        echo "=== POD SCHEDULING RESULTS ==="
        kubectl get pods -n integration-test -o wide
        
        echo "=== SCHEDULER LOGS ==="
        kubectl logs -n chronos-system -l app.kubernetes.io/name=chronos-kubernetes-scheduler --tail=50
        
        echo "=== SCHEDULING EVENTS ==="
        kubectl get events -n integration-test --field-selector reason=Scheduled
        
        echo "=== FAILED SCHEDULING EVENTS ==="  
        kubectl get events -n integration-test --field-selector reason=FailedScheduling

    - name: Print scheduler logs
      run: |
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        echo "=== SCHEDULER DEPLOYMENT STATUS ==="
        kubectl get pods -n chronos-system -o wide
        kubectl get deployment -n chronos-system
        
        # Get all scheduler pod names
        SCHEDULER_PODS=$(kubectl get pods -n chronos-system -l app.kubernetes.io/name=chronos-kubernetes-scheduler -o jsonpath='{.items[*].metadata.name}')
        echo "Scheduler pods found: $SCHEDULER_PODS"
        
        # Find the leader by checking the lease resource or fallback to single pod
        echo "=== IDENTIFYING SCHEDULER LEADER ==="
        
        # Check if we have single replica (no leader election needed)
        POD_COUNT=$(echo $SCHEDULER_PODS | wc -w)
        echo "Number of scheduler pods: $POD_COUNT"
        
        if [ "$POD_COUNT" -eq 1 ]; then
          LEADER_POD=$(echo $SCHEDULER_PODS | awk '{print $1}')
          echo "✅ Single replica deployment - using pod: $LEADER_POD"
        else
          echo "Multiple replicas detected - checking for leader election lease..."
          
          # Check for leases (standard scheduler lease locations)
          LEASE_NAME="chronos-kubernetes-scheduler" 
          LEADER_IDENTITY=""
          
          # Try kube-system first (default scheduler lease location)
          LEADER_IDENTITY=$(kubectl get lease $LEASE_NAME -n kube-system -o jsonpath='{.spec.holderIdentity}' 2>/dev/null || echo "")
          
          # Try chronos-system if not found in kube-system
          if [ -z "$LEADER_IDENTITY" ]; then
            LEADER_IDENTITY=$(kubectl get lease $LEASE_NAME -n chronos-system -o jsonpath='{.spec.holderIdentity}' 2>/dev/null || echo "")
          fi
          
          # Show available leases for debugging
          echo "Available leases:"
          kubectl get leases -n kube-system | grep -i chronos || echo "  No chronos leases in kube-system"
          kubectl get leases -n chronos-system | grep -i chronos || echo "  No chronos leases in chronos-system"
          
          if [ -n "$LEADER_IDENTITY" ]; then
            echo "Leader identity from lease: $LEADER_IDENTITY"
            # Extract pod name (lease identity format: podname_namespace_uid)
            LEADER_POD=$(echo $LEADER_IDENTITY | cut -d'_' -f1)
            echo "✅ Leader pod identified via lease: $LEADER_POD"
            
            # Verify pod exists in our list
            if echo "$SCHEDULER_PODS" | grep -q "$LEADER_POD"; then
              echo "✅ Leader pod confirmed in scheduler pod list"
            else
              echo "⚠️ Leader pod from lease not found in current pods, using first pod"
              LEADER_POD=$(echo $SCHEDULER_PODS | awk '{print $1}')
            fi
          else
            echo "⚠️ No lease found, using first pod (leader election may be disabled)"
            LEADER_POD=$(echo $SCHEDULER_PODS | awk '{print $1}')
          fi
        fi
        
        echo "=== LEADER SCHEDULER LOGS ($LEADER_POD) ==="
        kubectl logs -n chronos-system $LEADER_POD
        
        echo "=== SCHEDULER STARTUP LOGS ==="
        kubectl logs -n chronos-system $LEADER_POD | grep -E "(Chronos|plugin|scheduler)" | head -20
        
        echo "=== CHRONOS PLUGIN INITIALIZATION ==="
        kubectl logs -n chronos-system $LEADER_POD | grep -i "Initializing Chronos plugin" || echo "No Chronos plugin initialization found"
        
        echo "=== POD SCHEDULING ACTIVITY ==="
        kubectl logs -n chronos-system $LEADER_POD | grep -E "(Attempting to schedule|evaluating|scoring)" | head -10
        
        echo "=== CHRONOS_SCORE LOGS FROM LEADER ==="
        kubectl logs -n chronos-system $LEADER_POD | grep "CHRONOS_SCORE:" || echo "No CHRONOS_SCORE logs found"
        
        echo "=== NORMALIZED SCORE LOGS FROM LEADER ==="  
        kubectl logs -n chronos-system $LEADER_POD | grep "NormalizedScore:" || echo "No NormalizedScore logs found"
        
        echo "=== SUCCESSFULLY BOUND LOGS FROM LEADER ==="
        kubectl logs -n chronos-system $LEADER_POD | grep "Successfully bound" || echo "No successful binding logs found"
        
        echo "=== TEST POD SCHEDULER VERIFICATION ==="
        for pod in $(kubectl get pods -n integration-test -o name); do
          POD_NAME=$(echo $pod | cut -d'/' -f2)
          SCHEDULER=$(kubectl get pod -n integration-test $POD_NAME -o jsonpath='{.spec.schedulerName}')
          ANNOTATIONS=$(kubectl get pod -n integration-test $POD_NAME -o jsonpath='{.metadata.annotations}' || echo "none")
          echo "Pod: $POD_NAME | Scheduler: $SCHEDULER | Annotations: $ANNOTATIONS"
        done
        
        echo "=== BASIC VALIDATION (INFORMATIONAL) ==="
        CHRONOS_PODS=$(kubectl get pods -n integration-test -o jsonpath='{.items[?(@.spec.schedulerName=="chronos-kubernetes-scheduler")].metadata.name}' | wc -w)
        TOTAL_PODS=$(kubectl get pods -n integration-test --no-headers | wc -l)
        echo "Pods scheduled by Chronos: $CHRONOS_PODS/$TOTAL_PODS"
        
        CHRONOS_SCORE_COUNT=$(kubectl logs -n chronos-system $LEADER_POD | grep -c "CHRONOS_SCORE:" || echo "0")
        echo "CHRONOS_SCORE log entries: $CHRONOS_SCORE_COUNT"
        
        NORMALIZED_SCORE_COUNT=$(kubectl logs -n chronos-system $LEADER_POD | grep -c "NormalizedScore:" || echo "0") 
        echo "NormalizedScore log entries: $NORMALIZED_SCORE_COUNT"
        
        BINDING_COUNT=$(kubectl logs -n chronos-system $LEADER_POD | grep -c "Successfully bound" || echo "0")
        echo "Successfully bound events: $BINDING_COUNT"
        
        echo "=== NODE ANALYSIS ==="
        NODE_COUNT=$(kubectl get nodes --no-headers | wc -l)
        READY_NODE_COUNT=$(kubectl get nodes --no-headers | grep -c " Ready ")
        echo "Total nodes in cluster: $NODE_COUNT (Ready: $READY_NODE_COUNT)"
        
        kubectl get nodes -o wide
        
        if [ "$READY_NODE_COUNT" -eq 1 ]; then
          echo "ℹ️  Single node cluster detected - this explains why no scoring logs appear"
          echo "   Kubernetes skips scoring phase when there's only one feasible node"
          echo "   This is normal behavior and indicates our scheduler is working correctly"
        elif [ "$READY_NODE_COUNT" -gt 1 ]; then
          echo "🎯 Multi-node cluster detected - CHRONOS_SCORE logs should now appear!"
          echo "   Expected: evaluatedNodes=$READY_NODE_COUNT, with scoring comparisons between nodes"
          echo "   Our custom scheduler plugin should rank nodes using bin-packing logic"
        else
          echo "⚠️  No ready nodes detected - cluster may still be initializing"
        fi
        
        echo "=== CRITICAL VALIDATION CHECKS ==="
        
        # Check 1: Scheduler pod must be running
        if [ -z "$LEADER_POD" ]; then
          echo "❌ FAIL: No scheduler leader pod identified"
          exit 1
        fi
        
        SCHEDULER_STATUS=$(kubectl get pod -n chronos-system $LEADER_POD -o jsonpath='{.status.phase}')
        if [ "$SCHEDULER_STATUS" != "Running" ]; then
          echo "❌ FAIL: Scheduler pod is not running (status: $SCHEDULER_STATUS)"
          exit 1
        fi
        echo "✅ PASS: Scheduler pod is running"
        
        # Check 2: If multiple replicas, ensure lease exists (leader election working)
        if [ "$POD_COUNT" -gt 1 ]; then
          # We should have found a leader via lease for multi-replica deployments
          if [ -z "$LEADER_IDENTITY" ]; then
            echo "❌ FAIL: Multiple scheduler replicas but no leader election lease found"
            echo "This indicates leader election is not working properly"
            exit 1
          fi
          echo "✅ PASS: Leader election working (lease found: $LEADER_IDENTITY)"
        else
          echo "✅ PASS: Single replica deployment (no leader election needed)"
        fi
        
        echo "🎉 All critical infrastructure checks passed!"
        echo "📊 Log metrics above are informational only and don't affect build status"

    - name: Cleanup
      if: always()
      run: |
        # Show final status
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        kubectl get pods -A || true
