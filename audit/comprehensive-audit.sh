#!/bin/bash

# Chronos Scheduler Comprehensive Audit Script
# This script performs detailed step-by-step testing of all scheduling scenarios

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Global variables
TEST_COUNT=0
PASSED_TESTS=0
FAILED_TESTS=0
AUDIT_LOG="audit-$(date +%Y%m%d-%H%M%S).log"

# Logging function
log() {
    echo -e "$1" | tee -a "$AUDIT_LOG"
}

# Test result tracking
test_result() {
    local test_name="$1"
    local result="$2"
    local details="$3"
    
    TEST_COUNT=$((TEST_COUNT + 1))
    if [ "$result" = "PASS" ]; then
        PASSED_TESTS=$((PASSED_TESTS + 1))
        log "${GREEN}‚úÖ TEST $TEST_COUNT PASSED: $test_name${NC}"
    else
        FAILED_TESTS=$((FAILED_TESTS + 1))
        log "${RED}‚ùå TEST $TEST_COUNT FAILED: $test_name${NC}"
    fi
    
    if [ -n "$details" ]; then
        log "   Details: $details"
    fi
    log ""
}

# Wait for pod to be scheduled
wait_for_pod_scheduled() {
    local pod_selector="$1"
    local timeout="${2:-30}"
    
    log "${YELLOW}‚è≥ Waiting for pod to be scheduled (timeout: ${timeout}s)...${NC}"
    
    for i in $(seq 1 $timeout); do
        if kubectl get pods -l "$pod_selector" --no-headers 2>/dev/null | grep -q "Running\|Pending\|ContainerCreating"; then
            sleep 2  # Give it a moment to fully schedule
            return 0
        fi
        sleep 1
    done
    
    log "${RED}‚ùå Pod not scheduled within $timeout seconds${NC}"
    return 1
}

# Get pod node assignment
get_pod_node() {
    local pod_selector="$1"
    kubectl get pods -l "$pod_selector" -o jsonpath='{.items[0].spec.nodeName}' 2>/dev/null || echo "UNKNOWN"
}

# Get scheduler logs for a specific pod
get_scheduler_logs_for_pod() {
    local pod_name="$1"
    local lines="${2:-50}"
    kubectl logs -l app.kubernetes.io/name=chronos-kubernetes-scheduler --tail=$lines | grep "$pod_name" || echo "No logs found for $pod_name"
}

# Analyze node utilization
analyze_node_utilization() {
    log "${BLUE}üìä Current Node Utilization Analysis:${NC}"
    log "====================================="
    
    # Get all nodes
    local nodes=$(kubectl get nodes -o jsonpath='{.items[*].metadata.name}')
    
    for node in $nodes; do
        local pod_count=$(kubectl get pods --all-namespaces --field-selector spec.nodeName=$node --no-headers 2>/dev/null | wc -l)
        local scheduler_pods=$(kubectl get pods -l test=audit --field-selector spec.nodeName=$node --no-headers 2>/dev/null | wc -l)
        
        log "${CYAN}Node: $node${NC}"
        log "  Total Pods: $pod_count"
        log "  Our Test Pods: $scheduler_pods"
        
        if [ $scheduler_pods -gt 0 ]; then
            log "  Our Jobs on this node:"
            kubectl get pods -l test=audit --field-selector spec.nodeName=$node -o custom-columns="JOB:.metadata.labels.batch\.kubernetes\.io/job-name,DURATION:.metadata.annotations.scheduling\.workload\.io/expected-duration-seconds,STATUS:.status.phase" --no-headers 2>/dev/null | while read job duration status; do
                log "    - $job: ${duration:-N/A}s ($status)"
            done
        fi
        log ""
    done
}

# Clean up function
cleanup() {
    log "${YELLOW}üßπ Cleaning up test resources...${NC}"
    kubectl delete jobs -l test=audit --ignore-not-found=true >/dev/null 2>&1
    kubectl delete pods -l test=audit --ignore-not-found=true >/dev/null 2>&1
}

# Check scheduler readiness
check_scheduler_ready() {
    log "${BLUE}üîç Checking Scheduler Readiness${NC}"
    log "==============================="
    
    local scheduler_ready=$(kubectl get pods -l app.kubernetes.io/name=chronos-kubernetes-scheduler -o jsonpath='{.items[0].status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "False")
    
    if [ "$scheduler_ready" = "True" ]; then
        test_result "Scheduler Readiness" "PASS" "Chronos scheduler is ready and running"
        
        # Get scheduler pod details
        local scheduler_pod=$(kubectl get pods -l app.kubernetes.io/name=chronos-kubernetes-scheduler -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
        local scheduler_node=$(kubectl get pods -l app.kubernetes.io/name=chronos-kubernetes-scheduler -o jsonpath='{.items[0].spec.nodeName}' 2>/dev/null)
        
        log "  Scheduler Pod: $scheduler_pod"
        log "  Running on Node: $scheduler_node"
        log ""
        
        return 0
    else
        test_result "Scheduler Readiness" "FAIL" "Chronos scheduler is not ready"
        return 1
    fi
}

# Scenario 1: Single Job Baseline
test_single_job_baseline() {
    log "${PURPLE}üß™ AUDIT SCENARIO 1: Single Job Baseline${NC}"
    log "========================================"
    
    cleanup
    
    # Deploy a single job
    cat << EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: audit-single-job
  labels:
    test: audit
    scenario: single
spec:
  template:
    metadata:
      annotations:
        scheduling.workload.io/expected-duration-seconds: "600"  # 10 minutes
      labels:
        test: audit
        scenario: single
    spec:
      schedulerName: chronos-kubernetes-scheduler
      restartPolicy: Never
      containers:
      - name: workload
        image: busybox:1.36
        command: ["sh", "-c", "echo 'Single job started on' \$(hostname) 'at' \$(date) && sleep 600"]
        resources:
          requests:
            cpu: 100m
            memory: 64Mi
EOF

    if wait_for_pod_scheduled "test=audit,scenario=single"; then
        local node=$(get_pod_node "test=audit,scenario=single")
        test_result "Single Job Scheduling" "PASS" "Job scheduled to node: $node"
        
        # Analyze scheduling decision
        log "${CYAN}üìã Scheduling Decision Analysis:${NC}"
        get_scheduler_logs_for_pod "audit-single-job" 20
        
        analyze_node_utilization
        
        # Store baseline for comparison
        echo "$node" > /tmp/audit-baseline-node
        
        log "${GREEN}‚úÖ Baseline established: Single 600s job on $node${NC}"
    else
        test_result "Single Job Scheduling" "FAIL" "Job failed to schedule within timeout"
        return 1
    fi
}

# Scenario 2: Bin-Packing Test - Short Job Should Consolidate
test_binpacking_consolidation() {
    log "${PURPLE}üß™ AUDIT SCENARIO 2: Bin-Packing Consolidation${NC}"
    log "=============================================="
    
    local baseline_node=$(cat /tmp/audit-baseline-node 2>/dev/null || echo "UNKNOWN")
    
    if [ "$baseline_node" = "UNKNOWN" ]; then
        test_result "Bin-Packing Setup" "FAIL" "No baseline job found"
        return 1
    fi
    
    # Deploy a short job that should fit within the existing 600s job
    cat << EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: audit-short-consolidate
  labels:
    test: audit
    scenario: consolidate
spec:
  template:
    metadata:
      annotations:
        scheduling.workload.io/expected-duration-seconds: "300"  # 5 minutes - should fit in 10min job
      labels:
        test: audit
        scenario: consolidate
    spec:
      schedulerName: chronos-kubernetes-scheduler
      restartPolicy: Never
      containers:
      - name: workload
        image: busybox:1.36
        command: ["sh", "-c", "echo 'Short consolidation job started on' \$(hostname) 'at' \$(date) && sleep 300"]
        resources:
          requests:
            cpu: 100m
            memory: 64Mi
EOF

    if wait_for_pod_scheduled "test=audit,scenario=consolidate"; then
        local node=$(get_pod_node "test=audit,scenario=consolidate")
        
        if [ "$node" = "$baseline_node" ]; then
            test_result "Bin-Packing Consolidation" "PASS" "Short job consolidated on same node: $node (Expected: bin-packing within existing 600s job)"
        else
            test_result "Bin-Packing Consolidation" "FAIL" "Short job went to different node: $node vs baseline: $baseline_node"
        fi
        
        # Analyze scheduling decision
        log "${CYAN}üìã Consolidation Scheduling Analysis:${NC}"
        get_scheduler_logs_for_pod "audit-short-consolidate" 30
        
        analyze_node_utilization
        
    else
        test_result "Bin-Packing Consolidation" "FAIL" "Job failed to schedule within timeout"
        return 1
    fi
}

# Scenario 3: Extension Test - Long Job Should Minimize Extension
test_extension_minimization() {
    log "${PURPLE}üß™ AUDIT SCENARIO 3: Extension Minimization${NC}"
    log "========================================="
    
    # Deploy a long job that exceeds existing work - should prefer node with longer remaining time
    cat << EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: audit-long-extend
  labels:
    test: audit
    scenario: extend
spec:
  template:
    metadata:
      annotations:
        scheduling.workload.io/expected-duration-seconds: "1200"  # 20 minutes - exceeds the 10min baseline
      labels:
        test: audit
        scenario: extend
    spec:
      schedulerName: chronos-kubernetes-scheduler
      restartPolicy: Never
      containers:
      - name: workload
        image: busybox:1.36
        command: ["sh", "-c", "echo 'Long extension job started on' \$(hostname) 'at' \$(date) && sleep 1200"]
        resources:
          requests:
            cpu: 100m
            memory: 64Mi
EOF

    if wait_for_pod_scheduled "test=audit,scenario=extend"; then
        local node=$(get_pod_node "test=audit,scenario=extend")
        local baseline_node=$(cat /tmp/audit-baseline-node 2>/dev/null || echo "UNKNOWN")
        
        # For extension minimization, it should prefer the node with more existing work
        # Since our algorithm prioritizes utilization when extending, let's analyze the decision
        test_result "Extension Minimization" "PASS" "Long job scheduled to node: $node (Expected: minimized extension impact)"
        
        # Analyze scheduling decision
        log "${CYAN}üìã Extension Scheduling Analysis:${NC}"
        get_scheduler_logs_for_pod "audit-long-extend" 30
        
        analyze_node_utilization
        
        # Explain the scheduling logic
        if [ "$node" = "$baseline_node" ]; then
            log "${GREEN}‚úÖ GOOD: Long job chose same node - utilizing existing capacity${NC}"
        else
            log "${YELLOW}‚ÑπÔ∏è  INFO: Long job chose different node - may be optimizing for utilization${NC}"
        fi
        
    else
        test_result "Extension Minimization" "FAIL" "Job failed to schedule within timeout"
        return 1
    fi
}

# Scenario 4: Empty Node Avoidance Test
test_empty_node_avoidance() {
    log "${PURPLE}üß™ AUDIT SCENARIO 4: Empty Node Avoidance${NC}"
    log "======================================="
    
    # First, let's check current node distribution
    analyze_node_utilization
    
    # Deploy another job that should prefer busy nodes over empty ones
    cat << EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: audit-empty-avoid
  labels:
    test: audit
    scenario: avoid-empty
spec:
  template:
    metadata:
      annotations:
        scheduling.workload.io/expected-duration-seconds: "180"  # 3 minutes - short job
      labels:
        test: audit
        scenario: avoid-empty
    spec:
      schedulerName: chronos-kubernetes-scheduler
      restartPolicy: Never
      containers:
      - name: workload
        image: busybox:1.36
        command: ["sh", "-c", "echo 'Empty node avoidance job started on' \$(hostname) 'at' \$(date) && sleep 180"]
        resources:
          requests:
            cpu: 100m
            memory: 64Mi
EOF

    if wait_for_pod_scheduled "test=audit,scenario=avoid-empty"; then
        local node=$(get_pod_node "test=audit,scenario=avoid-empty")
        
        # Check if this node already has our test jobs (non-empty)
        local existing_jobs=$(kubectl get pods -l test=audit --field-selector spec.nodeName=$node --no-headers 2>/dev/null | wc -l)
        
        if [ $existing_jobs -gt 1 ]; then
            test_result "Empty Node Avoidance" "PASS" "Job chose busy node: $node (has $existing_jobs jobs total)"
        else
            test_result "Empty Node Avoidance" "PARTIAL" "Job chose node: $node (needs analysis of node state)"
        fi
        
        # Analyze scheduling decision
        log "${CYAN}üìã Empty Node Avoidance Analysis:${NC}"
        get_scheduler_logs_for_pod "audit-empty-avoid" 30
        
        analyze_node_utilization
        
    else
        test_result "Empty Node Avoidance" "FAIL" "Job failed to schedule within timeout"
        return 1
    fi
}

# Scenario 5: No Annotation Handling
test_no_annotation_handling() {
    log "${PURPLE}üß™ AUDIT SCENARIO 5: No Annotation Handling${NC}"
    log "========================================"
    
    # Deploy a job without duration annotation
    cat << EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: audit-no-annotation
  labels:
    test: audit
    scenario: no-annotation
spec:
  template:
    metadata:
      labels:
        test: audit
        scenario: no-annotation
    spec:
      schedulerName: chronos-kubernetes-scheduler
      restartPolicy: Never
      containers:
      - name: workload
        image: busybox:1.36
        command: ["sh", "-c", "echo 'No annotation job started on' \$(hostname) 'at' \$(date) && sleep 120"]
        resources:
          requests:
            cpu: 100m
            memory: 64Mi
EOF

    if wait_for_pod_scheduled "test=audit,scenario=no-annotation"; then
        local node=$(get_pod_node "test=audit,scenario=no-annotation")
        test_result "No Annotation Handling" "PASS" "Job without annotation scheduled to node: $node"
        
        # Analyze scheduling decision
        log "${CYAN}üìã No Annotation Scheduling Analysis:${NC}"
        get_scheduler_logs_for_pod "audit-no-annotation" 30
        
        analyze_node_utilization
        
    else
        test_result "No Annotation Handling" "FAIL" "Job failed to schedule within timeout"
        return 1
    fi
}

# Scenario 6: Multiple Short Jobs Consolidation
test_multiple_short_consolidation() {
    log "${PURPLE}üß™ AUDIT SCENARIO 6: Multiple Short Jobs Consolidation${NC}"
    log "==================================================="
    
    # Clean up first
    cleanup
    sleep 5
    
    # Deploy multiple short jobs in sequence and watch consolidation
    local jobs=("audit-multi-1:240" "audit-multi-2:180" "audit-multi-3:300")
    local first_node=""
    
    for job_spec in "${jobs[@]}"; do
        IFS=':' read -r job_name duration <<< "$job_spec"
        
        log "${YELLOW}üì¶ Deploying $job_name with ${duration}s duration...${NC}"
        
        cat << EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: $job_name
  labels:
    test: audit
    scenario: multi-short
spec:
  template:
    metadata:
      annotations:
        scheduling.workload.io/expected-duration-seconds: "$duration"
      labels:
        test: audit
        scenario: multi-short
    spec:
      schedulerName: chronos-kubernetes-scheduler
      restartPolicy: Never
      containers:
      - name: workload
        image: busybox:1.36
        command: ["sh", "-c", "echo '$job_name started on' \$(hostname) 'at' \$(date) && sleep $duration"]
        resources:
          requests:
            cpu: 100m
            memory: 64Mi
EOF
        
        if wait_for_pod_scheduled "test=audit,scenario=multi-short" 20; then
            local node=$(get_pod_node "batch.kubernetes.io/job-name=$job_name")
            log "${GREEN}‚úÖ $job_name scheduled to: $node${NC}"
            
            if [ -z "$first_node" ]; then
                first_node="$node"
            fi
            
            # Brief analysis after each job
            analyze_node_utilization
            
            # Small delay to see sequential behavior
            sleep 3
        else
            test_result "Multiple Short Jobs - $job_name" "FAIL" "Job failed to schedule within timeout"
        fi
    done
    
    # Final consolidation analysis
    local total_jobs=$(kubectl get pods -l test=audit,scenario=multi-short --no-headers 2>/dev/null | wc -l)
    local jobs_on_first_node=$(kubectl get pods -l test=audit,scenario=multi-short --field-selector spec.nodeName=$first_node --no-headers 2>/dev/null | wc -l)
    
    if [ $jobs_on_first_node -eq $total_jobs ] && [ $total_jobs -eq 3 ]; then
        test_result "Multiple Short Jobs Consolidation" "PASS" "All 3 short jobs consolidated on same node: $first_node"
    elif [ $jobs_on_first_node -gt 1 ]; then
        test_result "Multiple Short Jobs Consolidation" "PARTIAL" "$jobs_on_first_node out of $total_jobs jobs consolidated on $first_node"
    else
        test_result "Multiple Short Jobs Consolidation" "FAIL" "Jobs spread across multiple nodes - consolidation failed"
    fi
}

# Final comprehensive analysis
comprehensive_analysis() {
    log "${BLUE}üìä COMPREHENSIVE FINAL ANALYSIS${NC}"
    log "================================="
    
    # Overall cluster state
    log "${CYAN}üñ•Ô∏è  Final Cluster State:${NC}"
    kubectl get jobs -l test=audit --sort-by=.metadata.name
    echo ""
    kubectl get pods -l test=audit -o wide --sort-by=.spec.nodeName
    echo ""
    
    # Node distribution summary
    analyze_node_utilization
    
    # Scheduler effectiveness metrics
    local total_pods=$(kubectl get pods -l test=audit --no-headers 2>/dev/null | wc -l)
    local nodes_used=$(kubectl get pods -l test=audit -o jsonpath='{.items[*].spec.nodeName}' | tr ' ' '\n' | sort | uniq | wc -l)
    
    log "${CYAN}üìà Scheduling Effectiveness Metrics:${NC}"
    log "  Total Test Pods Scheduled: $total_pods"
    log "  Nodes Used: $nodes_used"
    log "  Consolidation Ratio: $(echo "scale=2; $total_pods / $nodes_used" | bc 2>/dev/null || echo "N/A")"
    
    # Algorithm behavior summary
    log "${CYAN}üß† Algorithm Behavior Summary:${NC}"
    log "  Bin-Packing: Jobs with compatible durations should consolidate"
    log "  Extension Minimization: Long jobs should minimize node commitment extensions"  
    log "  Empty Node Avoidance: Jobs should prefer busy nodes over empty ones"
    log "  Annotation Handling: Jobs without annotations should still be scheduled"
}

# Generate final report
generate_report() {
    log "${BLUE}üìã AUDIT REPORT SUMMARY${NC}"
    log "========================"
    log "Total Tests Run: $TEST_COUNT"
    log "Passed: ${GREEN}$PASSED_TESTS${NC}"
    log "Failed: ${RED}$FAILED_TESTS${NC}"
    log "Success Rate: $(echo "scale=1; $PASSED_TESTS * 100 / $TEST_COUNT" | bc 2>/dev/null || echo "N/A")%"
    log ""
    log "Full audit log saved to: $AUDIT_LOG"
    
    if [ $FAILED_TESTS -eq 0 ]; then
        log "${GREEN}üéâ ALL TESTS PASSED - Chronos Scheduler is working perfectly!${NC}"
    else
        log "${YELLOW}‚ö†Ô∏è  Some tests failed - Review the audit log for details${NC}"
    fi
}

# Main audit execution
main() {
    log "${PURPLE}üöÄ CHRONOS SCHEDULER COMPREHENSIVE AUDIT${NC}"
    log "========================================"
    log "Timestamp: $(date)"
    log "Audit Log: $AUDIT_LOG"
    log ""
    
    # Pre-flight checks
    if ! check_scheduler_ready; then
        log "${RED}‚ùå Scheduler not ready. Aborting audit.${NC}"
        exit 1
    fi
    
    # Sequential scenario testing
    test_single_job_baseline
    sleep 10
    
    test_binpacking_consolidation  
    sleep 10
    
    test_extension_minimization
    sleep 10
    
    test_empty_node_avoidance
    sleep 10
    
    test_no_annotation_handling
    sleep 10
    
    test_multiple_short_consolidation
    sleep 10
    
    # Final analysis
    comprehensive_analysis
    
    # Generate report
    generate_report
    
    # Cleanup
    log "${YELLOW}üßπ Cleaning up audit resources...${NC}"
    cleanup
    
    log "${GREEN}‚úÖ Audit completed successfully!${NC}"
}

# Run main function
main "$@"
